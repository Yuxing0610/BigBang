{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Model\n",
    "\n",
    "1. used the pretrianed Bert-base model to get the initial embeddings\n",
    "2. use two linear layers to deal with the multilable task\n",
    "    - one linear layer cope with the 8-class classification task that predict the current speaker given a line.\n",
    "    - the other linear layer cope with the 9-class classification task that predict the next speaker given a line.\n",
    "    - the total loss of the model is the sum of the cross-entropy loss of these two tasks.\n",
    "3. Tried both balanced-training and unbalanced-training\n",
    "4. the results are shown in the last two cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from torch.optim.lr_scheduler import LambdaLR, StepLR, MultiStepLR, ExponentialLR, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, recall_score, classification_report\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "labels = {\n",
    "    \"Sheldon\" : 0,\n",
    "    \"Penny\" : 1,\n",
    "    \"Leonard\" : 2,\n",
    "    \"Raj\" : 3,\n",
    "    \"Howard\" : 4,\n",
    "    \"Amy\" : 5,\n",
    "    \"Bernadette\" : 6,\n",
    "    \"Secondary\" : 7,\n",
    "    \"End\" : 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.cur_labels = [labels[label] for label in df['cur_speaker_label']]\n",
    "        self.next_labels = [labels[label] for label in df['next_speaker_label']]\n",
    "        self.lines = [tokenizer(line, padding = 'max_length', max_length = 512, truncation= True, return_tensors = \"pt\") for line in df[\"raw_line\"]]\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cur_labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        return {\"cur\": np.array(self.cur_labels[idx]), \"next\" : np.array(self.next_labels[idx])}\n",
    "    \n",
    "    def get_batch_lines(self, idx):\n",
    "        return self.lines[idx]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.get_batch_lines(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, dropout = 0.5):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.cur = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(768, 8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.next = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(768, 9),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, mask):\n",
    "        _, cls_embedding = self.bert(input_ids = input_ids, attention_mask = mask, return_dict = False)\n",
    "        return {\n",
    "            'cur' : self.cur(cls_embedding),\n",
    "            'next' : self.next(cls_embedding)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_acc(criterion_cur, criterion_next, output, label, device):\n",
    "    label['cur'] = label['cur'].type(torch.LongTensor)\n",
    "    label['next'] = label['next'].type(torch.LongTensor)\n",
    "    label['cur'] = label['cur'].to(device)\n",
    "    label['next'] = label['next'].to(device)\n",
    "    cur_loss = criterion_cur(output['cur'], label['cur'])\n",
    "    next_loss = criterion_next(output['next'], label['next'])\n",
    "    loss = cur_loss + next_loss\n",
    "    cur_acc = (output['cur'].argmax(dim=1) == label['cur']).sum().item()\n",
    "    next_acc = (output['next'].argmax(dim=1) == label['next']).sum().item()\n",
    "    return loss, {'cur' : cur_loss, 'next' : next_loss}, {'cur' : cur_acc, 'next': next_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, train_data, val_data, learning_rate, epochs, check_point, batch_size):\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size = batch_size)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  \n",
    "    #The weight here is set by the value we get from the \"Fasttext_unsupervised_training.ipynb\"\n",
    "    criterion_cur = nn.CrossEntropyLoss(weight = torch.tensor([0.48333895, 0.90484957, 0.71429165, 1.27128289, 1.06559713, 1.87805677, 2.52688014, 1.29423714]))\n",
    "    criterion_next = nn.CrossEntropyLoss(weight = torch.tensor([0.5671942,  0.75431904, 0.59435462, 1.2725995,  1.03293404, 1.61986817, 2.12974311, 1.1452633, 1.94252484]))\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.2)\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion_cur = criterion_cur.cuda()\n",
    "        criterion_next = criterion_next.cuda()\n",
    "    \n",
    "    i = 0\n",
    "    train_iteration = []\n",
    "    val_iteration = []\n",
    "\n",
    "    total_train_loss = []\n",
    "    cur_train_loss = []\n",
    "    next_train_loss = []\n",
    "    cur_train_acc = []\n",
    "    next_train_acc = []\n",
    "\n",
    "    total_val_loss = []\n",
    "    cur_val_loss = []\n",
    "    next_val_loss = []\n",
    "    cur_val_acc = []\n",
    "    next_val_acc = []\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_loss_train = 0\n",
    "        cur_loss_train = 0\n",
    "        next_loss_train = 0\n",
    "        cur_acc_train = 0\n",
    "        next_acc_train = 0\n",
    "        \n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "            \n",
    "            output = model(input_id, mask)\n",
    "                    \n",
    "            batch_loss, categorical_loss, categorical_acc = get_loss_acc(criterion_cur, criterion_next, output, train_label, device)\n",
    "                \n",
    "            total_loss_train += batch_loss.item()\n",
    "            cur_loss_train += categorical_loss['cur'].item()\n",
    "            next_loss_train += categorical_loss['next'].item()\n",
    "            cur_acc_train += categorical_acc['cur']\n",
    "            next_acc_train += categorical_acc['next']\n",
    "            \n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_iteration.append(i)\n",
    "            total_train_loss.append(batch_loss.item()/batch_size)\n",
    "            cur_train_loss.append(categorical_loss['cur'].item()/batch_size)\n",
    "            next_train_loss.append(categorical_loss['next'].item()/batch_size)\n",
    "            cur_train_acc.append(categorical_acc['cur']/batch_size)\n",
    "            next_train_acc.append(categorical_acc['next']/batch_size)\n",
    "\n",
    "            i+=1\n",
    "\n",
    "            if i%check_point==0:\n",
    "                total_loss_val = 0\n",
    "                cur_loss_val = 0\n",
    "                next_loss_val = 0\n",
    "                cur_acc_val = 0\n",
    "                next_acc_val = 0\n",
    "            \n",
    "                with torch.no_grad():\n",
    "                        \n",
    "                    for val_input, val_label in val_dataloader:\n",
    "                        mask = val_input['attention_mask'].to(device)\n",
    "                        input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "        \n",
    "                        output = model(input_id, mask)\n",
    "\n",
    "                        batch_loss, categorical_loss, categorical_acc = get_loss_acc(criterion_cur, criterion_next, output, val_label, device)\n",
    "                        total_loss_val += batch_loss.item()\n",
    "                        cur_loss_val += categorical_loss['cur'].item()\n",
    "                        next_loss_val += categorical_loss['next'].item()\n",
    "                        cur_acc_val += categorical_acc['cur']\n",
    "                        next_acc_val += categorical_acc['next']\n",
    "\n",
    "                print(\n",
    "                    f'''Epochs: {epoch_num + 1} \n",
    "                    | Iterations: {i}\n",
    "                    | Total Train Loss: {total_loss_train / (check_point*batch_size): .3f} \n",
    "                    | Cur Train Loss : {cur_loss_train / (check_point*batch_size): .3f} \n",
    "                    | Next Train Loss : {next_loss_train / (check_point*batch_size): .3f}\n",
    "                    | Cur Train Accuracy : {cur_acc_train / (check_point*batch_size): .3f}\n",
    "                    | Next Train Accuracy: {next_acc_train / (check_point*batch_size): .3f} \n",
    "                    | Total Val Loss: {total_loss_val / len(val_data): .3f} \n",
    "                    | Cur Val Loss : {cur_loss_val / len(val_data): .3f} \n",
    "                    | Next Val Loss : {next_loss_val / len(val_data): .3f}\n",
    "                    | Cur Val Accuracy : {cur_acc_val / len(val_data): .3f}\n",
    "                    | Next Val Accuracy: {next_acc_val / len(val_data): .3f}''')  \n",
    "                    \n",
    "                val_iteration.append(i)\n",
    "                total_val_loss.append(total_loss_val / len(val_data))\n",
    "                cur_val_loss.append(cur_loss_val / len(val_data))\n",
    "                next_val_loss.append(next_loss_val / len(val_data))\n",
    "                cur_val_acc.append(cur_acc_val / len(val_data))\n",
    "                next_val_acc.append(next_acc_val / len(val_data))\n",
    "\n",
    "                total_loss_train = 0\n",
    "                cur_loss_train = 0\n",
    "                next_loss_train = 0\n",
    "                cur_acc_train = 0\n",
    "                next_acc_train = 0\n",
    "                \n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    \n",
    "    total_loss_val = 0\n",
    "    cur_loss_val = 0\n",
    "    next_loss_val = 0\n",
    "    cur_acc_val = 0\n",
    "    next_acc_val = 0\n",
    "        \n",
    "    with torch.no_grad():\n",
    "                    \n",
    "        for val_input, val_label in val_dataloader:\n",
    "            mask = val_input['attention_mask'].to(device)\n",
    "            input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "    \n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            batch_loss, categorical_loss, categorical_acc = get_loss_acc(criterion_cur, criterion_next, output, val_label, device)\n",
    "            total_loss_val += batch_loss.item()\n",
    "            cur_loss_val += categorical_loss['cur'].item()\n",
    "            next_loss_val += categorical_loss['next'].item()\n",
    "            cur_acc_val += categorical_acc['cur']\n",
    "            next_acc_val += categorical_acc['next']\n",
    "    print(\n",
    "        f'''Total Val Loss: {total_loss_val / len(val_data): .3f} \n",
    "            | Cur Val Loss : {cur_loss_val / len(val_data): .3f} \n",
    "            | Next Val Loss : {next_loss_val / len(val_data): .3f}\n",
    "            | Cur Val Accuracy : {cur_acc_val / len(val_data): .3f}\n",
    "            | Next Val Accuracy: {next_acc_val / len(val_data): .3f}''')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_csv(\"data/test_data.csv\")\n",
    "df_train = pd.read_csv(\"data/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  7%|▋         | 500/7203 [02:26<17:44:05,  9.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 \n",
      "                    | Iterations: 500\n",
      "                    | Total Train Loss:  0.860 \n",
      "                    | Cur Train Loss :  0.419 \n",
      "                    | Next Train Loss :  0.442\n",
      "                    | Cur Train Accuracy :  0.151\n",
      "                    | Next Train Accuracy:  0.129 \n",
      "                    | Total Val Loss:  0.858 \n",
      "                    | Cur Val Loss :  0.418 \n",
      "                    | Next Val Loss :  0.441\n",
      "                    | Cur Val Accuracy :  0.204\n",
      "                    | Next Val Accuracy:  0.164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1000/7203 [05:04<17:07:06,  9.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 \n",
      "                    | Iterations: 1000\n",
      "                    | Total Train Loss:  0.853 \n",
      "                    | Cur Train Loss :  0.414 \n",
      "                    | Next Train Loss :  0.440\n",
      "                    | Cur Train Accuracy :  0.242\n",
      "                    | Next Train Accuracy:  0.176 \n",
      "                    | Total Val Loss:  0.852 \n",
      "                    | Cur Val Loss :  0.411 \n",
      "                    | Next Val Loss :  0.441\n",
      "                    | Cur Val Accuracy :  0.251\n",
      "                    | Next Val Accuracy:  0.156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 1500/7203 [07:44<16:05:55, 10.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 \n",
      "                    | Iterations: 1500\n",
      "                    | Total Train Loss:  0.849 \n",
      "                    | Cur Train Loss :  0.409 \n",
      "                    | Next Train Loss :  0.440\n",
      "                    | Cur Train Accuracy :  0.255\n",
      "                    | Next Train Accuracy:  0.188 \n",
      "                    | Total Val Loss:  0.851 \n",
      "                    | Cur Val Loss :  0.411 \n",
      "                    | Next Val Loss :  0.439\n",
      "                    | Cur Val Accuracy :  0.253\n",
      "                    | Next Val Accuracy:  0.202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 2000/7203 [10:24<14:30:00, 10.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 \n",
      "                    | Iterations: 2000\n",
      "                    | Total Train Loss:  0.846 \n",
      "                    | Cur Train Loss :  0.408 \n",
      "                    | Next Train Loss :  0.437\n",
      "                    | Cur Train Accuracy :  0.259\n",
      "                    | Next Train Accuracy:  0.212 \n",
      "                    | Total Val Loss:  0.847 \n",
      "                    | Cur Val Loss :  0.408 \n",
      "                    | Next Val Loss :  0.439\n",
      "                    | Cur Val Accuracy :  0.259\n",
      "                    | Next Val Accuracy:  0.201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 2500/7203 [13:05<13:18:47, 10.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 \n",
      "                    | Iterations: 2500\n",
      "                    | Total Train Loss:  0.844 \n",
      "                    | Cur Train Loss :  0.406 \n",
      "                    | Next Train Loss :  0.437\n",
      "                    | Cur Train Accuracy :  0.259\n",
      "                    | Next Train Accuracy:  0.199 \n",
      "                    | Total Val Loss:  0.843 \n",
      "                    | Cur Val Loss :  0.405 \n",
      "                    | Next Val Loss :  0.438\n",
      "                    | Cur Val Accuracy :  0.279\n",
      "                    | Next Val Accuracy:  0.199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 3000/7203 [15:44<11:49:22, 10.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 \n",
      "                    | Iterations: 3000\n",
      "                    | Total Train Loss:  0.839 \n",
      "                    | Cur Train Loss :  0.403 \n",
      "                    | Next Train Loss :  0.436\n",
      "                    | Cur Train Accuracy :  0.282\n",
      "                    | Next Train Accuracy:  0.208 \n",
      "                    | Total Val Loss:  0.846 \n",
      "                    | Cur Val Loss :  0.407 \n",
      "                    | Next Val Loss :  0.439\n",
      "                    | Cur Val Accuracy :  0.274\n",
      "                    | Next Val Accuracy:  0.182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 3500/7203 [18:25<10:24:33, 10.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 \n",
      "                    | Iterations: 3500\n",
      "                    | Total Train Loss:  0.837 \n",
      "                    | Cur Train Loss :  0.402 \n",
      "                    | Next Train Loss :  0.435\n",
      "                    | Cur Train Accuracy :  0.304\n",
      "                    | Next Train Accuracy:  0.202 \n",
      "                    | Total Val Loss:  0.840 \n",
      "                    | Cur Val Loss :  0.403 \n",
      "                    | Next Val Loss :  0.437\n",
      "                    | Cur Val Accuracy :  0.296\n",
      "                    | Next Val Accuracy:  0.195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 4000/7203 [21:05<8:56:55, 10.06s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 \n",
      "                    | Iterations: 4000\n",
      "                    | Total Train Loss:  0.836 \n",
      "                    | Cur Train Loss :  0.403 \n",
      "                    | Next Train Loss :  0.434\n",
      "                    | Cur Train Accuracy :  0.303\n",
      "                    | Next Train Accuracy:  0.226 \n",
      "                    | Total Val Loss:  0.840 \n",
      "                    | Cur Val Loss :  0.403 \n",
      "                    | Next Val Loss :  0.437\n",
      "                    | Cur Val Accuracy :  0.298\n",
      "                    | Next Val Accuracy:  0.191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 4500/7203 [23:46<7:43:29, 10.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 \n",
      "                    | Iterations: 4500\n",
      "                    | Total Train Loss:  0.834 \n",
      "                    | Cur Train Loss :  0.401 \n",
      "                    | Next Train Loss :  0.433\n",
      "                    | Cur Train Accuracy :  0.285\n",
      "                    | Next Train Accuracy:  0.198 \n",
      "                    | Total Val Loss:  0.840 \n",
      "                    | Cur Val Loss :  0.401 \n",
      "                    | Next Val Loss :  0.438\n",
      "                    | Cur Val Accuracy :  0.278\n",
      "                    | Next Val Accuracy:  0.160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 5000/7203 [26:27<6:14:00, 10.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 \n",
      "                    | Iterations: 5000\n",
      "                    | Total Train Loss:  0.832 \n",
      "                    | Cur Train Loss :  0.399 \n",
      "                    | Next Train Loss :  0.433\n",
      "                    | Cur Train Accuracy :  0.301\n",
      "                    | Next Train Accuracy:  0.197 \n",
      "                    | Total Val Loss:  0.838 \n",
      "                    | Cur Val Loss :  0.401 \n",
      "                    | Next Val Loss :  0.437\n",
      "                    | Cur Val Accuracy :  0.310\n",
      "                    | Next Val Accuracy:  0.194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 5500/7203 [29:08<4:49:54, 10.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 \n",
      "                    | Iterations: 5500\n",
      "                    | Total Train Loss:  0.827 \n",
      "                    | Cur Train Loss :  0.397 \n",
      "                    | Next Train Loss :  0.430\n",
      "                    | Cur Train Accuracy :  0.295\n",
      "                    | Next Train Accuracy:  0.220 \n",
      "                    | Total Val Loss:  0.837 \n",
      "                    | Cur Val Loss :  0.401 \n",
      "                    | Next Val Loss :  0.436\n",
      "                    | Cur Val Accuracy :  0.309\n",
      "                    | Next Val Accuracy:  0.185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 6000/7203 [31:50<3:30:09, 10.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 \n",
      "                    | Iterations: 6000\n",
      "                    | Total Train Loss:  0.823 \n",
      "                    | Cur Train Loss :  0.391 \n",
      "                    | Next Train Loss :  0.431\n",
      "                    | Cur Train Accuracy :  0.315\n",
      "                    | Next Train Accuracy:  0.204 \n",
      "                    | Total Val Loss:  0.835 \n",
      "                    | Cur Val Loss :  0.399 \n",
      "                    | Next Val Loss :  0.436\n",
      "                    | Cur Val Accuracy :  0.296\n",
      "                    | Next Val Accuracy:  0.179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 6500/7203 [34:33<1:58:52, 10.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 \n",
      "                    | Iterations: 6500\n",
      "                    | Total Train Loss:  0.822 \n",
      "                    | Cur Train Loss :  0.392 \n",
      "                    | Next Train Loss :  0.430\n",
      "                    | Cur Train Accuracy :  0.312\n",
      "                    | Next Train Accuracy:  0.209 \n",
      "                    | Total Val Loss:  0.829 \n",
      "                    | Cur Val Loss :  0.395 \n",
      "                    | Next Val Loss :  0.434\n",
      "                    | Cur Val Accuracy :  0.301\n",
      "                    | Next Val Accuracy:  0.179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 7000/7203 [37:15<35:04, 10.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 \n",
      "                    | Iterations: 7000\n",
      "                    | Total Train Loss:  0.819 \n",
      "                    | Cur Train Loss :  0.389 \n",
      "                    | Next Train Loss :  0.430\n",
      "                    | Cur Train Accuracy :  0.328\n",
      "                    | Next Train Accuracy:  0.203 \n",
      "                    | Total Val Loss:  0.826 \n",
      "                    | Cur Val Loss :  0.393 \n",
      "                    | Next Val Loss :  0.433\n",
      "                    | Cur Val Accuracy :  0.310\n",
      "                    | Next Val Accuracy:  0.190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7203/7203 [38:08<00:00,  3.15it/s]\n",
      "  4%|▍         | 297/7203 [01:49<19:35:30, 10.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 \n",
      "                    | Iterations: 7500\n",
      "                    | Total Train Loss:  0.475 \n",
      "                    | Cur Train Loss :  0.225 \n",
      "                    | Next Train Loss :  0.250\n",
      "                    | Cur Train Accuracy :  0.207\n",
      "                    | Next Train Accuracy:  0.148 \n",
      "                    | Total Val Loss:  0.820 \n",
      "                    | Cur Val Loss :  0.388 \n",
      "                    | Next Val Loss :  0.432\n",
      "                    | Cur Val Accuracy :  0.306\n",
      "                    | Next Val Accuracy:  0.207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 797/7203 [04:31<18:12:01, 10.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 \n",
      "                    | Iterations: 8000\n",
      "                    | Total Train Loss:  0.795 \n",
      "                    | Cur Train Loss :  0.370 \n",
      "                    | Next Train Loss :  0.426\n",
      "                    | Cur Train Accuracy :  0.368\n",
      "                    | Next Train Accuracy:  0.230 \n",
      "                    | Total Val Loss:  0.820 \n",
      "                    | Cur Val Loss :  0.388 \n",
      "                    | Next Val Loss :  0.432\n",
      "                    | Cur Val Accuracy :  0.317\n",
      "                    | Next Val Accuracy:  0.206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1297/7203 [07:11<16:31:43, 10.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 \n",
      "                    | Iterations: 8500\n",
      "                    | Total Train Loss:  0.792 \n",
      "                    | Cur Train Loss :  0.369 \n",
      "                    | Next Train Loss :  0.423\n",
      "                    | Cur Train Accuracy :  0.358\n",
      "                    | Next Train Accuracy:  0.244 \n",
      "                    | Total Val Loss:  0.821 \n",
      "                    | Cur Val Loss :  0.388 \n",
      "                    | Next Val Loss :  0.432\n",
      "                    | Cur Val Accuracy :  0.341\n",
      "                    | Next Val Accuracy:  0.205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 1797/7203 [09:51<15:10:31, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 \n",
      "                    | Iterations: 9000\n",
      "                    | Total Train Loss:  0.788 \n",
      "                    | Cur Train Loss :  0.367 \n",
      "                    | Next Train Loss :  0.421\n",
      "                    | Cur Train Accuracy :  0.366\n",
      "                    | Next Train Accuracy:  0.244 \n",
      "                    | Total Val Loss:  0.818 \n",
      "                    | Cur Val Loss :  0.387 \n",
      "                    | Next Val Loss :  0.431\n",
      "                    | Cur Val Accuracy :  0.314\n",
      "                    | Next Val Accuracy:  0.196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 2297/7203 [12:31<13:55:42, 10.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 \n",
      "                    | Iterations: 9500\n",
      "                    | Total Train Loss:  0.784 \n",
      "                    | Cur Train Loss :  0.363 \n",
      "                    | Next Train Loss :  0.421\n",
      "                    | Cur Train Accuracy :  0.374\n",
      "                    | Next Train Accuracy:  0.248 \n",
      "                    | Total Val Loss:  0.819 \n",
      "                    | Cur Val Loss :  0.388 \n",
      "                    | Next Val Loss :  0.432\n",
      "                    | Cur Val Accuracy :  0.332\n",
      "                    | Next Val Accuracy:  0.218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 2797/7203 [15:13<12:35:10, 10.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 \n",
      "                    | Iterations: 10000\n",
      "                    | Total Train Loss:  0.789 \n",
      "                    | Cur Train Loss :  0.366 \n",
      "                    | Next Train Loss :  0.423\n",
      "                    | Cur Train Accuracy :  0.376\n",
      "                    | Next Train Accuracy:  0.232 \n",
      "                    | Total Val Loss:  0.816 \n",
      "                    | Cur Val Loss :  0.385 \n",
      "                    | Next Val Loss :  0.432\n",
      "                    | Cur Val Accuracy :  0.311\n",
      "                    | Next Val Accuracy:  0.206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 3297/7203 [17:55<11:13:43, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 \n",
      "                    | Iterations: 10500\n",
      "                    | Total Train Loss:  0.792 \n",
      "                    | Cur Train Loss :  0.366 \n",
      "                    | Next Train Loss :  0.426\n",
      "                    | Cur Train Accuracy :  0.360\n",
      "                    | Next Train Accuracy:  0.227 \n",
      "                    | Total Val Loss:  0.814 \n",
      "                    | Cur Val Loss :  0.383 \n",
      "                    | Next Val Loss :  0.431\n",
      "                    | Cur Val Accuracy :  0.318\n",
      "                    | Next Val Accuracy:  0.202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 3797/7203 [20:37<9:48:55, 10.37s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 \n",
      "                    | Iterations: 11000\n",
      "                    | Total Train Loss:  0.786 \n",
      "                    | Cur Train Loss :  0.365 \n",
      "                    | Next Train Loss :  0.421\n",
      "                    | Cur Train Accuracy :  0.372\n",
      "                    | Next Train Accuracy:  0.232 \n",
      "                    | Total Val Loss:  0.814 \n",
      "                    | Cur Val Loss :  0.384 \n",
      "                    | Next Val Loss :  0.429\n",
      "                    | Cur Val Accuracy :  0.321\n",
      "                    | Next Val Accuracy:  0.204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 4297/7203 [23:23<8:35:38, 10.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 \n",
      "                    | Iterations: 11500\n",
      "                    | Total Train Loss:  0.779 \n",
      "                    | Cur Train Loss :  0.357 \n",
      "                    | Next Train Loss :  0.422\n",
      "                    | Cur Train Accuracy :  0.395\n",
      "                    | Next Train Accuracy:  0.230 \n",
      "                    | Total Val Loss:  0.812 \n",
      "                    | Cur Val Loss :  0.382 \n",
      "                    | Next Val Loss :  0.430\n",
      "                    | Cur Val Accuracy :  0.329\n",
      "                    | Next Val Accuracy:  0.206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4797/7203 [26:06<6:50:58, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 \n",
      "                    | Iterations: 12000\n",
      "                    | Total Train Loss:  0.790 \n",
      "                    | Cur Train Loss :  0.365 \n",
      "                    | Next Train Loss :  0.424\n",
      "                    | Cur Train Accuracy :  0.382\n",
      "                    | Next Train Accuracy:  0.213 \n",
      "                    | Total Val Loss:  0.811 \n",
      "                    | Cur Val Loss :  0.383 \n",
      "                    | Next Val Loss :  0.428\n",
      "                    | Cur Val Accuracy :  0.307\n",
      "                    | Next Val Accuracy:  0.226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 5297/7203 [28:46<5:26:42, 10.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 \n",
      "                    | Iterations: 12500\n",
      "                    | Total Train Loss:  0.781 \n",
      "                    | Cur Train Loss :  0.361 \n",
      "                    | Next Train Loss :  0.420\n",
      "                    | Cur Train Accuracy :  0.382\n",
      "                    | Next Train Accuracy:  0.240 \n",
      "                    | Total Val Loss:  0.811 \n",
      "                    | Cur Val Loss :  0.381 \n",
      "                    | Next Val Loss :  0.429\n",
      "                    | Cur Val Accuracy :  0.338\n",
      "                    | Next Val Accuracy:  0.208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 5797/7203 [31:28<4:00:29, 10.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 \n",
      "                    | Iterations: 13000\n",
      "                    | Total Train Loss:  0.773 \n",
      "                    | Cur Train Loss :  0.355 \n",
      "                    | Next Train Loss :  0.418\n",
      "                    | Cur Train Accuracy :  0.394\n",
      "                    | Next Train Accuracy:  0.245 \n",
      "                    | Total Val Loss:  0.811 \n",
      "                    | Cur Val Loss :  0.382 \n",
      "                    | Next Val Loss :  0.428\n",
      "                    | Cur Val Accuracy :  0.324\n",
      "                    | Next Val Accuracy:  0.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 6297/7203 [34:09<2:33:10, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 \n",
      "                    | Iterations: 13500\n",
      "                    | Total Train Loss:  0.769 \n",
      "                    | Cur Train Loss :  0.354 \n",
      "                    | Next Train Loss :  0.416\n",
      "                    | Cur Train Accuracy :  0.391\n",
      "                    | Next Train Accuracy:  0.254 \n",
      "                    | Total Val Loss:  0.813 \n",
      "                    | Cur Val Loss :  0.382 \n",
      "                    | Next Val Loss :  0.431\n",
      "                    | Cur Val Accuracy :  0.339\n",
      "                    | Next Val Accuracy:  0.205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 6797/7203 [36:49<1:08:37, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 \n",
      "                    | Iterations: 14000\n",
      "                    | Total Train Loss:  0.772 \n",
      "                    | Cur Train Loss :  0.356 \n",
      "                    | Next Train Loss :  0.415\n",
      "                    | Cur Train Accuracy :  0.392\n",
      "                    | Next Train Accuracy:  0.257 \n",
      "                    | Total Val Loss:  0.811 \n",
      "                    | Cur Val Loss :  0.382 \n",
      "                    | Next Val Loss :  0.429\n",
      "                    | Cur Val Accuracy :  0.331\n",
      "                    | Next Val Accuracy:  0.214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7203/7203 [38:33<00:00,  3.11it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Val Loss:  0.811 \n",
      "            | Cur Val Loss :  0.382 \n",
      "            | Next Val Loss :  0.429\n",
      "            | Cur Val Accuracy :  0.330\n",
      "            | Next Val Accuracy:  0.209\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/processed_lines.csv\")\n",
    "\n",
    "EPOCHS = 2\n",
    "model = BertClassifier()\n",
    "LR = 1e-5\n",
    "run_model(model, df_train, df_val, LR, EPOCHS, 500, 5)\n",
    "torch.save(model.state_dict(), \"Models/Balanced_MultiLabel_Bert.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data):\n",
    "    model.eval()\n",
    "    test = Dataset(test_data)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size = 5)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  \n",
    "    #The weight here is set by the value we get from the \"Fasttext_unsupervised_training.ipynb\"\n",
    "    criterion_cur = nn.CrossEntropyLoss()\n",
    "    criterion_next = nn.CrossEntropyLoss()\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion_cur = criterion_cur.cuda()\n",
    "        criterion_next = criterion_next.cuda()\n",
    "    \n",
    "    total_loss_val = 0\n",
    "    cur_loss_val = 0\n",
    "    next_loss_val = 0\n",
    "    cur_acc_val = 0\n",
    "    next_acc_val = 0\n",
    "\n",
    "    cur_speaker_pred = []\n",
    "    cur_speaker_y = []\n",
    "    next_speaker_pred = []\n",
    "    next_speaker_y = []\n",
    "        \n",
    "    with torch.no_grad():\n",
    "                    \n",
    "        for val_input, val_label in test_dataloader:\n",
    "            mask = val_input['attention_mask'].to(device)\n",
    "            input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "    \n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            \n",
    "            cur_speaker_pred += torch.argmax(output['cur'], dim = 1).tolist()\n",
    "            next_speaker_pred += torch.argmax(output['next'], dim = 1).tolist()\n",
    "            cur_speaker_y += val_label['cur'].tolist()\n",
    "            next_speaker_y += val_label['next'].tolist()\n",
    "\n",
    "\n",
    "            batch_loss, categorical_loss, categorical_acc = get_loss_acc(criterion_cur, criterion_next, output, val_label, device)\n",
    "            total_loss_val += batch_loss.item()\n",
    "            cur_loss_val += categorical_loss['cur'].item()\n",
    "            next_loss_val += categorical_loss['next'].item()\n",
    "            cur_acc_val += categorical_acc['cur']\n",
    "            next_acc_val += categorical_acc['next']\n",
    "    print(\n",
    "        f'''Total Test Loss: {total_loss_val / len(test_data): .3f} \n",
    "        | Cur_speaker Test Loss : {cur_loss_val / len(test_data): .3f} \n",
    "        | Next_speaker Test Loss : {next_loss_val / len(test_data): .3f}\n",
    "        | Cur_speaker Test Accuracy : {cur_acc_val / len(test_data): .3f}\n",
    "        | Next_speaker Test Accuracy: {next_acc_val / len(test_data): .3f}''') \n",
    "\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    print(\"The classification report for current speaker prediction:\")\n",
    "    cur_speaker_m = classification_report(cur_speaker_y, cur_speaker_pred, target_names = [\"Sheldon\", \"Penny\", \"Leonard\", \"Raj\",\"Howard\",\"Amy\",\"Bernadette\",\"Secondary\"])\n",
    "    print(cur_speaker_m)\n",
    "\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    print(\"The classification report for next speaker prediction:\")\n",
    "    next_speaker_m = classification_report(next_speaker_y, next_speaker_pred, target_names = [\"Sheldon\", \"Penny\", \"Leonard\", \"Raj\",\"Howard\",\"Amy\",\"Bernadette\",\"Secondary\", \"Ending\"])\n",
    "    print(next_speaker_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Test Loss:  0.758 \n",
      "        | Cur_speaker Test Loss :  0.355 \n",
      "        | Next_speaker Test Loss :  0.403\n",
      "        | Cur_speaker Test Accuracy :  0.365\n",
      "        | Next_speaker Test Accuracy:  0.269\n",
      "====================================================================\n",
      "The classification report for current speaker prediction:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Sheldon       0.50      0.72      0.59       475\n",
      "       Penny       0.34      0.42      0.38       251\n",
      "     Leonard       0.25      0.50      0.33       298\n",
      "         Raj       0.32      0.12      0.17       195\n",
      "      Howard       0.28      0.18      0.22       250\n",
      "         Amy       0.50      0.01      0.01       135\n",
      "  Bernadette       0.50      0.02      0.04        94\n",
      "   Secondary       0.33      0.12      0.18       198\n",
      "\n",
      "    accuracy                           0.36      1896\n",
      "   macro avg       0.38      0.26      0.24      1896\n",
      "weighted avg       0.37      0.36      0.32      1896\n",
      "\n",
      "====================================================================\n",
      "The classification report for next speaker prediction:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Sheldon       0.31      0.52      0.39       376\n",
      "       Penny       0.25      0.26      0.25       295\n",
      "     Leonard       0.24      0.50      0.32       321\n",
      "         Raj       0.27      0.07      0.11       160\n",
      "      Howard       0.23      0.16      0.19       203\n",
      "         Amy       0.47      0.05      0.10       131\n",
      "  Bernadette       0.00      0.00      0.00       110\n",
      "   Secondary       0.28      0.16      0.21       183\n",
      "      Ending       0.00      0.00      0.00       117\n",
      "\n",
      "    accuracy                           0.27      1896\n",
      "   macro avg       0.23      0.19      0.17      1896\n",
      "weighted avg       0.25      0.27      0.23      1896\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\sony\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Anaconda\\envs\\sony\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Anaconda\\envs\\sony\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model_test = BertClassifier()\n",
    "model_test.load_state_dict(torch.load(\"Models/MultiLabel_Bert.pt\"))\n",
    "test_model(model_test, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Test Loss:  0.780 \n",
      "        | Cur_speaker Test Loss :  0.359 \n",
      "        | Next_speaker Test Loss :  0.421\n",
      "        | Cur_speaker Test Accuracy :  0.342\n",
      "        | Next_speaker Test Accuracy:  0.228\n",
      "====================================================================\n",
      "The classification report for current speaker prediction:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Sheldon       0.60      0.57      0.59       475\n",
      "       Penny       0.31      0.53      0.39       251\n",
      "     Leonard       0.29      0.27      0.28       298\n",
      "         Raj       0.26      0.22      0.24       195\n",
      "      Howard       0.24      0.18      0.21       250\n",
      "         Amy       0.19      0.21      0.20       135\n",
      "  Bernadette       0.09      0.09      0.09        94\n",
      "   Secondary       0.26      0.20      0.22       198\n",
      "\n",
      "    accuracy                           0.34      1896\n",
      "   macro avg       0.28      0.28      0.28      1896\n",
      "weighted avg       0.34      0.34      0.34      1896\n",
      "\n",
      "====================================================================\n",
      "The classification report for next speaker prediction:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Sheldon       0.29      0.49      0.36       376\n",
      "       Penny       0.35      0.12      0.18       295\n",
      "     Leonard       0.26      0.16      0.20       321\n",
      "         Raj       0.14      0.10      0.12       160\n",
      "      Howard       0.18      0.24      0.21       203\n",
      "         Amy       0.15      0.15      0.15       131\n",
      "  Bernadette       0.13      0.15      0.14       110\n",
      "   Secondary       0.20      0.30      0.24       183\n",
      "      Ending       0.16      0.03      0.04       117\n",
      "\n",
      "    accuracy                           0.23      1896\n",
      "   macro avg       0.20      0.19      0.18      1896\n",
      "weighted avg       0.23      0.23      0.21      1896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "balanced_model_test = BertClassifier()\n",
    "balanced_model_test.load_state_dict(torch.load(\"Models/Balanced_MultiLabel_Bert.pt\"))\n",
    "test_model(balanced_model_test, df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sony",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
