{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model\n",
    "\n",
    "1. Prepare the data for training and validation, here we only use lines with more than 5 vocabularies, also lower case all characters and eliminate all punctuations. \n",
    "2. Use the unsupervised training method of fasttext with **skipgram** to train the embeddings of vocabularies.\n",
    "3. Try different methods to aggregate multiple tokens' embeddings to one sentence embedding, using the mean aggregation at last\n",
    "4. Train two seperate logistic regression classifier for two different tasks (predict the current speaker/next speaker)\n",
    "5. The results are shown in the last two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, classification_report\n",
    "from utils import word2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/processed_lines.csv\")\n",
    "train_document = \"./data/Fasttext/fasttext_unsupervised_train.txt\"\n",
    "labels = {\n",
    "    \"Sheldon\" : 0,\n",
    "    \"Penny\" : 1,\n",
    "    \"Leonard\" : 2,\n",
    "    \"Raj\" : 3,\n",
    "    \"Howard\" : 4,\n",
    "    \"Amy\" : 5,\n",
    "    \"Bernadette\" : 6,\n",
    "    \"Secondary\" : 7,\n",
    "    \"End\" : 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsupervised_fasttext_data_prep(df):\n",
    "    '''\n",
    "    Prepare data for fasttext unsupervised training\n",
    "    '''\n",
    "    labels_cur_speaker = []\n",
    "    labels_next_speaker = []\n",
    "    with open(train_document, \"w\") as f:\n",
    "        for index, row in df.iterrows():\n",
    "            line = str(row.raw_line).lower().strip()\n",
    "            line = line.translate(str.maketrans({key:None for key in string.punctuation}))\n",
    "            if len(line.split(\" \")) >= 6:\n",
    "                f.write(line + '\\n')\n",
    "                labels_cur_speaker.append(row.cur_speaker_label)\n",
    "                labels_next_speaker.append(row.next_speaker_label)\n",
    "    return labels_cur_speaker, labels_next_speaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_cur_speaker, labels_next_speaker = unsupervised_fasttext_data_prep(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the embeddings for tokens\n",
    "embedding_model = fasttext.train_unsupervised(train_document, model = \"skipgram\", dim = 150, epoch = 300, lr = 0.1, ws=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34406, 150)\n"
     ]
    }
   ],
   "source": [
    "# get the embedding for each line from the tokens' embedding\n",
    "\n",
    "vocabulary = embedding_model.words\n",
    "word_embeddings = np.array([embedding_model[word] for word in vocabulary])\n",
    "vector_dict = dict(zip(vocabulary, word_embeddings))\n",
    "with open(train_document) as f:\n",
    "    content = f.readlines()\n",
    "documents = np.array([x.strip() for x in content])\n",
    "aggregated_doc_vectors = word2text(documents, vector_dict, word_embeddings.shape[1], 'mean')\n",
    "print(aggregated_doc_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48333895 0.90484957 0.71429165 1.27128289 1.06559713 1.87805677\n",
      " 2.52688014 1.29423714]\n",
      "[0.5671942  0.75431904 0.59435462 1.2725995  1.03293404 1.61986817\n",
      " 2.12974311 1.1452633  1.94252484]\n"
     ]
    }
   ],
   "source": [
    "# Get the weight of each class, they are not used in this notebook, because logistic regression api offers automatic weights calculation, but these data are used in the bert notebook\n",
    "\n",
    "y_cur_speaker = np.array([labels[x] for x in labels_cur_speaker])\n",
    "y_next_speaker = np.array([labels[x] for x in labels_next_speaker])\n",
    "cur_speaker_weight = sklearn.utils.class_weight.compute_class_weight(class_weight = \"balanced\", classes= np.unique(y_cur_speaker), y= y_cur_speaker)\n",
    "next_speaker_weight = sklearn.utils.class_weight.compute_class_weight(class_weight = \"balanced\", classes= np.unique(y_next_speaker), y= y_next_speaker)\n",
    "print(cur_speaker_weight)\n",
    "print(next_speaker_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_speaker_data = np.concatenate((y_cur_speaker.reshape(len(documents), 1), aggregated_doc_vectors), axis = 1)\n",
    "next_speaker_data = np.concatenate((y_next_speaker.reshape(len(documents), 1), aggregated_doc_vectors), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classfier(data, class_weight):\n",
    "    np.random.shuffle(data)\n",
    "    data_train = data[ : int(0.9*len(data))]\n",
    "    data_val = data[int(0.9*len(data)): ]\n",
    "    train_y = data_train[:,0]\n",
    "    train_x = data_train[:, 1:]\n",
    "    val_y = data_val[:,0]\n",
    "    val_x = data_val[:, 1:]\n",
    "\n",
    "    model = LogisticRegression(multi_class = \"ovr\", max_iter = 2500, class_weight = \"balanced\", solver = 'liblinear', C=5.0)\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    predict_val = model.predict(val_x)\n",
    "    t = classification_report(val_y, predict_val, target_names = [\"Sheldon\", \"Penny\", \"Leonard\", \"Raj\",\"Howard\",\"Amy\",\"Bernadette\",\"Secondary\",\"End\"])\n",
    "    print(t)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Sheldon       0.49      0.55      0.52       891\n",
      "       Penny       0.27      0.37      0.31       478\n",
      "     Leonard       0.24      0.18      0.21       601\n",
      "         Raj       0.25      0.20      0.22       347\n",
      "      Howard       0.20      0.12      0.15       389\n",
      "         Amy       0.11      0.09      0.10       243\n",
      "  Bernadette       0.11      0.22      0.15       190\n",
      "   Secondary       0.17      0.14      0.15       302\n",
      "\n",
      "    accuracy                           0.29      3441\n",
      "   macro avg       0.23      0.23      0.23      3441\n",
      "weighted avg       0.28      0.29      0.28      3441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cur_model = train_classfier(cur_speaker_data, cur_speaker_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Sheldon       0.30      0.32      0.31       697\n",
      "       Penny       0.15      0.10      0.12       508\n",
      "     Leonard       0.26      0.17      0.20       630\n",
      "         Raj       0.08      0.09      0.09       288\n",
      "      Howard       0.12      0.09      0.10       352\n",
      "         Amy       0.11      0.13      0.12       231\n",
      "  Bernadette       0.10      0.22      0.13       198\n",
      "   Secondary       0.16      0.17      0.16       343\n",
      "         End       0.09      0.15      0.11       194\n",
      "\n",
      "    accuracy                           0.17      3441\n",
      "   macro avg       0.15      0.16      0.15      3441\n",
      "weighted avg       0.18      0.17      0.17      3441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "next_model = train_classfier(next_speaker_data, next_speaker_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sony",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
